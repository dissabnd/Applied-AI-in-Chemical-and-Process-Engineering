{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biNn2f-fllo3"
   },
   "source": [
    "<br><br>\n",
    "<p style=\"font-size:24px; color:#216594;text-align:center;\">\n",
    "Applied AI in Chemical and Process Engineering\n",
    "</p>\n",
    "\n",
    "<p style=\"font-size:30px; color:black;text-align:center;\">\n",
    "    Bulding ML pipeline for Reactor data\n",
    "</p>\n",
    "\n",
    "\n",
    "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Week 3-4**\n",
    "\n",
    "This notebook will build a pipeline to prepare data and develop an ML algorithm\n",
    "\n",
    "Content:\n",
    "\n",
    "Part 1\n",
    "\n",
    "*   Data Preperation\n",
    "*   Data Cleaning\n",
    "*   Exploratory Data Analysis (EDA)\n",
    "\n",
    "Part 2\n",
    "*   Normalization (only principle)\n",
    "*   Model training, validation and testing\n",
    "*   Predictions\n",
    "*   Model explanation with XAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethanol Production - Sugar Industry\n",
    "\n",
    "![Process Flow Diagram](./assets/etoh_flowdiagram.png)\n",
    "\n",
    "\n",
    "**Our goal is to model residual sugar. The model can be later used to optimize the fermentation process**\n",
    "\n",
    "**Data**\n",
    "- Temperature ¬∞C: Fermentation tank\n",
    "- pH: Fermentation tank\n",
    "- Yeast Concentration g/L: Fermentation tank\n",
    "- Sugar Concentration w/v: Dilution tank (input for fermenation)\n",
    "- Residual Sugar g/L: Post-fermentation\n",
    "\n",
    "\n",
    "Note: The data is simualted data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Gfmsy52vMvl"
   },
   "source": [
    "# Data Preperation\n",
    "\n",
    "**Prompt:**\n",
    "Load data from below github\n",
    "https://raw.githubusercontent.com/dissabnd/Applied-AI-in-Chemical-and-Process-Engineering/refs/heads/main/data/Ethanol_Molasses_Dataset.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompt:**\n",
    "\n",
    "Print first 15 rows of the table and the dimensions of the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULfIiwZy_gQo"
   },
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hP3qTzztpMHf"
   },
   "source": [
    "## Check for missing data\n",
    "\n",
    "**Prompt:**\n",
    "Check if the table has missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "me53KmW0pnDv"
   },
   "source": [
    "## Fill the missing data wiht median using imputer\n",
    "\n",
    "**Prompt:**\n",
    "Fill the missing data with median value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26aIzJZGp6WF"
   },
   "source": [
    "## Check for duplicates\n",
    "\n",
    "**Prompt:**\n",
    "\n",
    "Check if it has duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Dara Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPNGHjzKsPDf"
   },
   "source": [
    "## Summary Stats\n",
    "\n",
    "**Prompt:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJ3om-gNoZoj"
   },
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ocp4y_YWtKqX"
   },
   "source": [
    "### Data Visualization with Box Plot\n",
    "\n",
    "\n",
    "![Box plot](./assets/boxplot.png)\n",
    "\n",
    "\n",
    "**Prompt:**\n",
    "\n",
    "Create box plots for all variables in multiple plots in single figure. show outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81HtOmOh91v1"
   },
   "source": [
    "### Data Visualization with Violine plot\n",
    "\n",
    "\n",
    "![Violin plot](./assets/violinplot.png)\n",
    "\n",
    "\n",
    "**Prompt:**\n",
    "\n",
    "Create violine plots for all variables in multiple plots in single figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VLTlntuD_CV3"
   },
   "source": [
    "### Data Visualization with Histogram plot\n",
    "\n",
    "**Prompt:**\n",
    "\n",
    "Create histogram plots for all variables in multiple plots in single figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VR4LOXiJdLpO"
   },
   "source": [
    "## Outliers\n",
    "\n",
    "**Strategy:** We will use statistical methods to remove outliers (IQR)\n",
    "\n",
    "**Prompt:**\n",
    "\n",
    "Remove all outliers of df using IQR method and create a table called dfclean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Znp941SifflJ"
   },
   "source": [
    "### Histogram plot with cleaned data\n",
    "\n",
    "**Prompt:**\n",
    "\n",
    "Create histogram plots for all variables in multiple plots in single figure using dfclean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ke1hfWul_saz"
   },
   "source": [
    "## Corelinearity\n",
    "\n",
    "**Prompt:** \n",
    "\n",
    "Print correlations matrix and do a plot a heatmap of correlation of dfclean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuTpK72qgLg0"
   },
   "source": [
    "# Data Prep for Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7R2roFzhOjs"
   },
   "source": [
    "## Define Features and Target\n",
    "\n",
    "**Prompt:**\n",
    "\n",
    "'Residual_Sugar' is Y variables and Temperature','pH','Yeast_Concentration','Sugar_Concentration' are features. Plot scatter plot between features and Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering with Data Scaling\n",
    "\n",
    "**Prompt:**\n",
    "\n",
    "Normalize X data using StandarScaler and plot histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "\n",
    "![Model guide](./assets/model_selection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwi_1ZsThWwX"
   },
   "source": [
    "# XGBoost Model\n",
    "\n",
    "\n",
    "**Note** Since this is tree model which does not require feature scaling in general, we **will not** use scaled data for model fitting\n",
    "\n",
    "\n",
    "<p style=\"font-size:25px; color:blue;text-align:left;\">\n",
    "    XGBoost\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "    üîò XGBoost is a machine learning algorithm that builds multiple decision trees one after another, where each new tree tries to fix the errors made by the previous trees.\n",
    "\n",
    "    üîò It uses a method called gradient boosting, which helps improve prediction accuracy by combining many weak models into a stronger one.\n",
    "    \n",
    "    üîòThe algorithm includes features to prevent overfitting, such as regularization and controlling how the trees grow.\n",
    "\n",
    "    üîò XGBoost can handle large datasets efficiently by using parallel processing to speed up training.\n",
    "\n",
    "    üîò It is widely used because it is fast, accurate, and works well for tasks like classification and regression, making it popular among beginners and experts alike.\n",
    "\n",
    "![xgboost](./assets/xgboost.png)\n",
    "\n",
    "References:\n",
    "\n",
    "[XGboost paper](https://arxiv.org/abs/1603.02754)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split\n",
    "\n",
    "![data split](./assets/datasplit.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompt**\n",
    "\n",
    "Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cw5TjikVhz7C"
   },
   "source": [
    "## Hyperparameter Optimization\n",
    "| Parameter             | Purpose                              | Simple Explanation | Default |\n",
    "|-----------------------|--------------------------------------|--------------------|---------|\n",
    "| `n_estimators`        | Number of decision trees             | More trees = more learning, but too many can overdo it. Like having more experts vote on the answer. <br> ‚úÖ **Typical tuned range:** 100‚Äì300 | `100` |\n",
    "| `max_depth`           | How deep each tree can go            | Controls complexity. Deeper = more detailed rules, but may memorize data. Like allowing more \"if-then\" steps. <br> ‚úÖ **Typical tuned range:** 3‚Äì10 | `6` |\n",
    "| `learning_rate`       | How fast the model learns            | Smaller = slow, steady improvement. Larger = fast but may overshoot. Like step size toward the goal. <br> ‚úÖ **Typical tuned range:** 0.01 ‚Äì 0.3 | `0.3` |\n",
    "| `subsample`           | % of data used for each tree         | Uses only part of the data per tree to avoid overfitting. Like asking different small groups to learn. <br> ‚úÖ **Typical tuned range:** 0.6 ‚Äì 1.0 | `1.0` |\n",
    "| `colsample_bytree`    | % of features used per tree          | Each tree uses only some columns (e.g., size, age, location). Prevents over-reliance on one feature. <br> ‚úÖ **Typical tuned range:** 0.6 ‚Äì 1.0 | `1.0` |\n",
    "| `min_child_weight`    | Minimum data in a prediction box     | Stops trees from splitting too small. ‚ÄúDon‚Äôt make a rule unless at least a few examples agree.‚Äù <br> ‚úÖ **Typical tuned range:** 1 ‚Äì 7 | `1` |\n",
    "| `gamma`               | Minimum improvement to split         | Only split if it clearly helps. Like saying: ‚ÄúOnly add a rule if it makes things meaningfully better.‚Äù <br> ‚úÖ **Typical tuned range:** 0 ‚Äì 0.5 | `0` |\n",
    "| `reg_alpha`           | Simplifies model (L1 penalty)        | Shrinks weak signals to zero. Helps when many inputs are noisy or irrelevant. <br> ‚úÖ **Typical tuned range:** 0 ‚Äì 1 | `0` |\n",
    "| `reg_lambda`          | Smooths predictions (L2 penalty)     | Keeps predictions stable by avoiding extreme values. Works like a safety brake. <br> ‚úÖ **Typical tuned range:** 1 ‚Äì 2 | `1` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "\n",
    "üîò Grid Search is like trying all possible combinations of settings to find the best one ‚Äî just like testing different oven temperatures and baking times to make the perfect cake. \n",
    "\n",
    "üîò It automatically tests every combination of hyperparameters (like learning_rate=0.1, max_depth=6) you specify, trains the model each time, and picks the one with the best performance. \n",
    "\n",
    "üîò It uses cross-validation (e.g., 5-fold) to ensure the result is reliable and not just lucky on one data split. \n",
    "\n",
    "üîò Think of it as \"brute-force tuning\" ‚Äî thorough, systematic, and great for finding optimal settings, but can be slow with too many parameters. \n",
    "\n",
    "*Example: If you test 5 values for max_depth, and 5 values for n_estimators, Grid Search will try all 5x5=25 combinations and tell you which works best.* \n",
    "     \n",
    "\n",
    "\n",
    "![grid](./assets/grid_search.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold Cross validation\n",
    "\n",
    "In K-fold cross-validation, the data is split into K equal parts (folds). The model is trained K times, each time leaving out one fold for testing and using the other K-1 folds for training. The final performance metric is averaged over all K runs. This approach uses the entire dataset for both training and testing, providing a more reliable estimate of model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompt:**\n",
    "\n",
    "Finetune below hyperparamters of xgboost with 5 fold cross validation. Print R2/RMSE of testing and training\n",
    "\n",
    "```\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'subsample': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "**Prompt:**\n",
    "Evaluate the model as below.\n",
    "1. Plot actual vs predicted values for both train and test\n",
    "2. Print R2/RMSE on the graph\n",
    "3. Plot residual for both training and testing\n",
    "\n",
    "\n",
    "## Residual Plot\n",
    "\n",
    "![residual](./assets/residualplot.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save final model\n",
    "\n",
    "**Prompt:** \n",
    "\n",
    "Save the model in pickle format"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "513788764cd0ec0f97313d5418a13e1ea666d16d72f976a8acadce25a5af2ffc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
